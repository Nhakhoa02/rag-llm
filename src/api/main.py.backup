"""
Main FastAPI application for the distributed indexing system.
"""

import asyncio
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Depends, Query, Path
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from pydantic import BaseModel
import requests
import json
from typing import cast

from ..config import settings
from ..utils.logging import get_logger
from ..utils.metrics import metrics_collector
from ..ingestion.processor import DocumentProcessor, ImageProcessor, TabularProcessor
from ..ingestion.chunker import TextChunker
from ..ai.gemini_client import GeminiClient, extract_gemini_text
from ..storage.distributed_storage_manager import create_distributed_storage_manager
from ..storage.distributed_vector_store import VectorNode
from ..models.base import BaseDocument
from ..models.document import Document as DocModel
from ..storage.auto_scaler import create_auto_scaler, ScalingThresholds

# Initialize components
logger = get_logger(__name__)
document_processor = DocumentProcessor()
image_processor = ImageProcessor()
tabular_processor = TabularProcessor()
text_chunker = TextChunker()
gemini_client = GeminiClient()

# Initialize distributed storage manager
storage_manager = create_distributed_storage_manager(
    nodes=[
        VectorNode("node1", "localhost", 8001),
        VectorNode("node2", "localhost", 8002),
        VectorNode("node3", "localhost", 8003)
    ],
    replication_factor=2,
    consistency_level="quorum",
    shard_count=8,
    vector_size=384
)

# Initialize auto-scaler
auto_scaler = create_auto_scaler(storage_manager)

async def determine_collections(document: BaseDocument, ai_metadata: Dict[str, Any]) -> List[str]:
    """
    Use AI to determine which collections this document should be stored in.
    Creates specialized collections based on content analysis.
    """
    try:
        # Base collections based on document type
        base_collections = [f"index_{document.type.value}"]
        
        # Analyze content for specialized collections
        content_preview = document.content[:1000] if document.content else "No content available"
        content_analysis_prompt = f"""
        Analyze this content and determine which specialized collections it should be stored in.
        
        Content Type: {document.type.value}
        Content Preview: {content_preview}...
        
        Available metadata: {ai_metadata}
        
        Consider creating specialized collections for:
        - Content topics (e.g., index_technology, index_finance, index_healthcare)
        - Content formats (e.g., index_reports, index_manuals, index_research)
        - Content domains (e.g., index_business, index_academic, index_creative)
        - Content languages (e.g., index_english, index_spanish, index_french)
        - Content sentiment (e.g., index_positive, index_negative, index_neutral)
        
        Return a JSON list of collection names (without 'index_' prefix):
        ["technology", "reports", "business", "english"]
        
        Guidelines:
        - Create 2-4 specialized collections
        - Use descriptive, lowercase names
        - Consider the main topics and characteristics
        - Don't include the base collection (already handled)
        """
        
        response = await gemini_client.generate_text(content_analysis_prompt, temperature=0.3)
        response_text = extract_gemini_text(response)
        
        # Parse the response
        try:
            if "[" in response_text and "]" in response_text:
                start = response_text.find("[")
                end = response_text.rfind("]") + 1
                json_str = response_text[start:end]
                specialized_collections = json.loads(json_str)
                
                # Add 'index_' prefix to specialized collections
                specialized_collections = [f"index_{col}" for col in specialized_collections if isinstance(col, str)]
                
                # Combine base and specialized collections
                all_collections = base_collections + specialized_collections
                logger.info(f"AI determined collections: {all_collections}")
                return all_collections
                
        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Failed to parse collection analysis: {e}")
        
        # Fallback to base collection
        return base_collections
        
    except Exception as e:
        logger.error(f"Collection determination failed: {e}")
        return [f"index_{document.type.value}"]

# Create FastAPI app
app = FastAPI(
    title="Distributed Indexing System",
    description="Scalable, fault-tolerant distributed indexing and search system",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.api.cors_origins,
    allow_credentials=True,
    allow_methods=settings.api.cors_methods,
    allow_headers=settings.api.cors_headers,
)

# Request/Response models
class UploadResponse(BaseModel):
    document_id: str
    status: str
    message: str
    chunks_created: int
    metadata: Dict[str, Any]
    content: str = ""

class SearchRequest(BaseModel):
    query: str
    index_names: List[str] = []
    limit: int = 10
    score_threshold: float = 0.7
    search_strategy: str = "hybrid"

class SearchResult(BaseModel):
    document_id: str
    content: str
    score: float
    metadata: Dict[str, Any]
    source_index: str

class SearchResponse(BaseModel):
    results: List[SearchResult]
    total_results: int
    query_analysis: Dict[str, Any]
    reasoning: Dict[str, Any]

class AskRequest(BaseModel):
    question: str
    index_names: List[str] = []
    limit: int = 10
    score_threshold: float = 0.5
    search_strategy: str = "hybrid"
    include_sources: bool = True

class AskResponse(BaseModel):
    answer: str
    confidence: float
    sources: List[SearchResult]
    reasoning: Dict[str, Any]
    query_analysis: Dict[str, Any]

class IndexInfo(BaseModel):
    name: str
    type: str
    size: int
    description: str
    status: str

# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint."""
    try:
        return {
            "status": "healthy",
            "components": {
                "distributed_storage": "healthy",
                "gemini": "healthy",
                "processors": "healthy"
            },
            "message": "RAG system is running"
        }
    except Exception as e:
        logger.error("Health check failed", error=str(e))
        raise HTTPException(status_code=503, detail="Service unhealthy")

# Upload endpoint
@app.post("/upload", response_model=UploadResponse)
async def upload_file(
    file: UploadFile = File(...),
    metadata: str = Form("{}"),
    chunk_strategy: str = Form("auto")
):
    """
    Upload and process a file for indexing.
    
    This endpoint handles the complete flow:
    1. File validation and processing
    2. Content extraction and chunking
    3. Metadata extraction using Gemini AI
    4. Vector embedding generation
    5. Distributed storage in Qdrant
    """
    try:
        import tempfile
        import os
        
        logger.info(f"Received upload: {file.filename}")
        # Parse metadata
        try:
            file_metadata = json.loads(metadata)
        except json.JSONDecodeError:
            file_metadata = {}
        
        # Save uploaded file temporarily
        filename = file.filename or "unknown_file"
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # Process file based on type
            file_extension = os.path.splitext(filename)[1].lower()
            logger.info(f"Processing file type: {file_extension}")
            if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']:
                document = image_processor.process_image(temp_file_path, file_metadata)
            elif file_extension in ['.csv', '.xlsx', '.xls', '.parquet', '.json']:
                document = tabular_processor.process_tabular(temp_file_path, file_metadata)
            else:
                document = document_processor.process_document(temp_file_path, file_metadata)
            logger.info(f"Document processed: {document.id}")
            
            # Extract additional metadata using Gemini
            ai_metadata = {}  # Initialize to empty dict
            if document.content:
                try:
                    ai_metadata = await gemini_client.extract_metadata(
                        document.content, 
                        document.type.value
                    )
                    document.metadata.update(ai_metadata)
                    logger.info(f"AI metadata extracted for document: {document.id}")
                except Exception as e:
                    logger.error(f"AI metadata extraction failed: {e}")
                    # Keep ai_metadata as empty dict
            else:
                logger.warning(f"No content extracted from document: {document.id}")
            
            # Chunk document if needed
            chunked_docs = text_chunker.chunk_document(document, chunk_strategy)
            logger.info(f"Document chunked: {len(chunked_docs)} chunks")
            
            # Ensure chunked_docs is List[Document] for upsert_documents
            if chunked_docs and hasattr(chunked_docs[0], 'dict'):
                # Convert BaseDocument to Document if needed
                chunked_docs = [DocModel(**doc.dict()) if hasattr(doc, 'dict') else doc for doc in chunked_docs]
            
            # Generate embeddings
            embeddings = await gemini_client.generate_embeddings(document.content)
            
            if embeddings:
                # Determine collection names using AI analysis
                collection_names = await determine_collections(document, ai_metadata)
                logger.info(f"AI determined collections: {collection_names}")
                
                # Store in multiple collections
                vector_size = 384  # Use 384 for sentence-transformers embeddings
                stored_collections = []
                
                for collection_name in collection_names:
                    try:
                        # Create collection if it doesn't exist
                        await storage_manager.create_index(collection_name, vector_size)
                        
                        # Store documents in this collection
                        success = await storage_manager.upsert_documents(
                            index_name=collection_name,
                            documents=cast(List[DocModel], chunked_docs),  # Cast to proper type
                            embeddings=embeddings
                        )
                        
                        if success:
                            stored_collections.append(collection_name)
                            logger.info(f"Document stored in collection: {collection_name}")
                        else:
                            logger.warning(f"Failed to store in collection: {collection_name}")
                            
                    except Exception as e:
                        logger.error(f"Error storing in collection {collection_name}: {e}")
                        continue
                
                if not stored_collections:
                    # Fallback to default collection
                    default_collection = f"index_{document.type.value}"
                    await storage_manager.create_index(default_collection, vector_size)
                    # Ensure proper type conversion
                    documents_to_store = [DocModel(**doc.dict()) if hasattr(doc, 'dict') else doc for doc in chunked_docs]
                    await storage_manager.upsert_documents(default_collection, documents_to_store, embeddings)
                    stored_collections = [default_collection]
                    logger.info(f"Fallback to default collection: {default_collection}")
                
                # Clean up temporary file
                os.unlink(temp_file_path)
                
                return UploadResponse(
                    document_id=document.id,
                    status="success",
                    message=f"Document processed and stored in {len(stored_collections)} collections",
                    chunks_created=len(chunked_docs),
                    metadata=document.metadata,
                    content=document.content[:500] + "..." if len(document.content) > 500 else document.content
                )
            else:
                raise HTTPException(status_code=500, detail="No embeddings generated")
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except Exception as e:
        logger.error("File upload failed", error=str(e), filename=file.filename)
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

# Search endpoint
@app.post("/search", response_model=SearchResponse)
async def search_documents(request: SearchRequest):
    """
    Search across multiple distributed indexes using Gemini AI for reasoning.
    
    This endpoint demonstrates the full distributed search flow:
    1. Query analysis using Gemini AI to determine relevant indexes
    2. Parallel search across multiple distributed indexes
    3. Result aggregation and ranking
    4. AI-powered reasoning about results
    """
    try:
        # Get available indexes
        collections = await storage_manager.list_indexes()
        available_indexes = [
            {
                "name": col["name"],
                "type": "vector",
                "size": col["vectors_count"],
                "description": f"Distributed vector index with {col['vectors_count']} vectors",
                "status": "active"
            }
            for col in collections
        ]
        available_collection_names = [idx["name"] for idx in available_indexes]
        
        # If no indexes specified, use AI to recommend
        if not request.index_names:
            # Use AI to analyze query and recommend indexes
            query_analysis_prompt = f"""
            Analyze this search query and recommend which indexes to search:
            
            Query: "{request.query}"
            Available indexes: {[idx["name"] for idx in available_indexes]}
            
            Consider:
            - Content type (document, image, tabular)
            - Topic relevance
            - Search intent
            
            Return a JSON list of recommended index names:
            ["index_document", "index_technology"]
            """
            
            try:
                response = await gemini_client.generate_text(query_analysis_prompt, temperature=0.3)
                response_text = extract_gemini_text(response)
                
                # Parse response
                if "[" in response_text and "]" in response_text:
                    start = response_text.find("[")
                    end = response_text.rfind("]") + 1
                    json_str = response_text[start:end]
                    recommended_indexes = json.loads(json_str)
                    
                    # Filter to available indexes
                    selected_indexes = [idx for idx in recommended_indexes if idx in available_collection_names]
                    if not selected_indexes:
                        selected_indexes = available_collection_names[:2]  # Fallback
                else:
                    selected_indexes = available_collection_names[:2]  # Fallback
                    
            except Exception as e:
                logger.warning(f"Query analysis failed: {e}")
                selected_indexes = available_collection_names[:2]  # Fallback
        else:
            selected_indexes = request.index_names
        
        # Search in selected indexes
        search_tasks = []
        for index_name in selected_indexes:
            if index_name in available_collection_names:
                task = storage_manager.search_documents(
                    index_name=index_name,
                    query=request.query,
                    limit=request.limit,
                    score_threshold=request.score_threshold
                )
                search_tasks.append((index_name, task))
        
        if not search_tasks:
            # Search in all available collections as fallback
            for col in collections:
                task = storage_manager.search_documents(
                    index_name=col["name"],
                    query=request.query,
                    limit=request.limit,
                    score_threshold=request.score_threshold
                )
                search_tasks.append((col["name"], task))
        
        # Execute searches in parallel
        search_results = []
        for index_name, task in search_tasks:
            try:
                results = await task
                for result in results:
                    result["source_index"] = index_name
                    search_results.append(result)
            except Exception as e:
                logger.warning(f"Search failed for index {index_name}", error=str(e))
        
        # Sort results by score
        search_results.sort(key=lambda x: x["score"], reverse=True)
        
        # Limit results
        search_results = search_results[:request.limit]
        
        # Use Gemini to reason about results
        reasoning = await gemini_client.reason_about_results(
            request.query, 
            search_results
        )
        
        # Format response
        formatted_results = [
            SearchResult(
                document_id=result["document_id"],
                content=result["content"],
                score=result["score"],
                metadata=result["metadata"],
                source_index=result["source_index"]
            )
            for result in search_results
        ]
        # Ensure query_analysis is defined
        if 'query_analysis' not in locals():
            query_analysis = {}
        return SearchResponse(
            results=formatted_results,
            total_results=len(formatted_results),
            query_analysis=query_analysis,
            reasoning=reasoning
        )
        
    except Exception as e:
        logger.error("Search failed", error=str(e), query=request.query)
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

# Ask endpoint - AI-powered question answering
@app.post("/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """
    Ask a question and get an AI-generated answer based on indexed content.
    
    This endpoint:
    1. Searches for relevant content across indexes
    2. Uses Gemini AI to generate a comprehensive answer
    3. Provides reasoning and confidence scores
    4. Includes source documents for transparency
    """
    try:
        # First, perform a search to find relevant content
        search_request = SearchRequest(
            query=request.question,
            index_names=request.index_names,
            limit=request.limit,
            score_threshold=request.score_threshold,
            search_strategy=request.search_strategy
        )
        
        # Get search results
        search_response = await search_documents(search_request)
        
        if not search_response.results:
            # No relevant content found
            return AskResponse(
                answer="I couldn't find any relevant information to answer your question. Please try rephrasing or uploading more relevant documents.",
                confidence=0.0,
                sources=[],
                reasoning={
                    "answer": "No relevant content found",
                    "confidence": 0.0,
                    "result_assessment": [],
                    "missing_info": ["No relevant documents found"],
                    "follow_up_queries": ["Try uploading relevant documents", "Rephrase your question"]
                },
                query_analysis=search_response.query_analysis
            )
        
        # Use Gemini to generate a comprehensive answer
        try:
            # Create a prompt for answer generation
            sources_text = "\n\n".join([
                f"Source {i+1} (Score: {result.score:.3f}):\n{result.content[:1000]}"
                for i, result in enumerate(search_response.results[:5])  # Top 5 sources
            ])
            
            answer_prompt = f"""
            Based on the following sources, please provide a comprehensive answer to the user's question.
            
            Question: "{request.question}"
            
            Sources:
            {sources_text}
            
            Please provide:
            1. A direct, comprehensive answer to the question
            2. Your confidence level (0-1) in the answer
            3. Reasoning for your answer
            4. Any limitations or uncertainties
            
            Format your response as JSON:
            {{
                "answer": "Your comprehensive answer here",
                "confidence": 0.85,
                "reasoning": "Explanation of how you arrived at this answer",
                "limitations": ["Any limitations or uncertainties"],
                "sources_used": [0, 1, 2]  // Indices of most relevant sources
            }}
            
            Guidelines:
            - Be direct and comprehensive in your answer
            - If the sources don't fully answer the question, acknowledge this
            - If there are conflicting information in sources, mention this
            - Provide specific details from the sources when relevant
            - Be honest about confidence levels and limitations
            """
            
            # Generate answer using Gemini
            answer_response = await gemini_client.generate_text(answer_prompt, temperature=0.3)
            
            # Parse the answer
            try:
                answer_text = extract_gemini_text(answer_response)
                # Extract JSON from response
                if "{" in answer_text and "}" in answer_text:
                    start = answer_text.find("{")
                    end = answer_text.rfind("}") + 1
                    json_str = answer_text[start:end]
                    answer_data = json.loads(json_str)
                    
                    answer = answer_data.get("answer", "I couldn't generate a proper answer.")
                    confidence = answer_data.get("confidence", 0.5)
                    reasoning_text = answer_data.get("reasoning", "Analysis completed")
                    limitations = answer_data.get("limitations", [])
                    sources_used = answer_data.get("sources_used", [])
                    
                else:
                    # Fallback if JSON parsing fails
                    answer = answer_text
                    confidence = 0.6
                    reasoning_text = "Answer generated from search results"
                    limitations = ["JSON parsing failed"]
                    sources_used = list(range(min(3, len(search_response.results))))
                    
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"Answer JSON parsing failed: {e}")
                answer = extract_gemini_text(answer_response)
                confidence = 0.5
                reasoning_text = "Answer generated from search results"
                limitations = ["JSON parsing failed"]
                sources_used = list(range(min(3, len(search_response.results))))
            
            # Prepare sources for response
            sources = search_response.results if request.include_sources else []
            
            # Create reasoning object
            reasoning = {
                "answer": reasoning_text,
                "confidence": confidence,
                "limitations": limitations,
                "sources_used": sources_used,
                "result_assessment": search_response.reasoning.get("result_assessment", []),
                "missing_info": search_response.reasoning.get("missing_info", []),
                "follow_up_queries": search_response.reasoning.get("follow_up_queries", [])
            }
            
            return AskResponse(
                answer=answer,
                confidence=confidence,
                sources=sources,
                reasoning=reasoning,
                query_analysis=search_response.query_analysis
            )
            
        except Exception as e:
            logger.error(f"Answer generation failed: {e}")
            # Fallback response
            return AskResponse(
                answer=f"I found some relevant information but couldn't generate a proper answer. Here are the search results: {search_response.results[0].content[:200]}...",
                confidence=0.3,
                sources=search_response.results if request.include_sources else [],
                reasoning={
                    "answer": "Answer generation failed, providing search results",
                    "confidence": 0.3,
                    "limitations": ["AI answer generation failed"],
                    "sources_used": [0],
                    "result_assessment": [],
                    "missing_info": [],
                    "follow_up_queries": []
                },
                query_analysis=search_response.query_analysis
            )
        
    except Exception as e:
        logger.error("Ask question failed", error=str(e), question=request.question)
        raise HTTPException(status_code=500, detail=f"Ask question failed: {str(e)}")

# Get indexes endpoint
@app.get("/indexes", response_model=List[IndexInfo])
async def get_indexes():
    """Get information about all available indexes."""
    try:
        collections = await storage_manager.list_indexes()
        
        indexes = []
        for col in collections:
            index_info = IndexInfo(
                name=col["name"],
                type="vector",
                size=col["vectors_count"],
                description=f"Distributed vector index with {col['vectors_count']} vectors",
                status="active"
            )
            indexes.append(index_info)
        
        return indexes
        
    except Exception as e:
        logger.error("Failed to get indexes", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get indexes: {str(e)}")

# Metrics endpoint
@app.get("/metrics")
async def get_metrics():
    """Get system metrics."""
    try:
        return metrics_collector.get_metrics()
    except Exception as e:
        logger.error("Failed to get metrics", error=str(e))
        raise HTTPException(status_code=500, detail="Failed to get metrics")

# Startup event
@app.on_event("startup")
async def startup_event():
    """Initialize system on startup."""
    logger.info("Starting distributed indexing system")
    
    # Initialize default collections
    try:
        default_collections = ["index_document", "index_image", "index_tabular"]
        for collection_name in default_collections:
            try:
                await storage_manager.create_index(collection_name, 384)
                logger.info(f"Created default collection: {collection_name}")
            except Exception:
                # Collection might already exist
                pass
    except Exception as e:
        logger.warning("Failed to initialize default collections", error=str(e))

# Shutdown event
@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown."""
    logger.info("Shutting down distributed indexing system")

@app.get("/documents")
async def list_documents(
    collection: str = Query(..., description="Collection name"),
    limit: int = Query(10, description="Number of documents to return")
):
    """
    List documents in a distributed collection.
    """
    try:
        # Not yet implemented in distributed manager
        raise NotImplementedError("Listing documents is not yet implemented in the distributed system.")
    except Exception as e:
        logger.error("Failed to list documents", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to list documents: {str(e)}")

@app.delete("/delete_index")
async def delete_index(collection: str = Query(..., description="Collection name to delete")):
    """
    Delete a collection (index) by name.
    """
    try:
        await storage_manager.delete_index(collection)
        logger.info(f"Collection deleted: {collection}")
        return {"status": "ok", "message": f"Collection '{collection}' deleted."}
    except Exception as e:
        logger.error(f"Failed to delete collection {collection}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete collection: {e}")

# Document inspection endpoints
@app.get("/document/{document_id}")
async def get_document_by_id(
    document_id: str = Path(..., description="Document ID to retrieve"),
    collection: str = Query(..., description="Collection name")
):
    """
    Get a specific document by its ID from a collection.
    """
    try:
        result = await storage_manager.get_document_by_id(collection, document_id)
        if result:
            return result
        else:
            raise HTTPException(status_code=404, detail="Document not found")
    except Exception as e:
        logger.error(f"Failed to get document {document_id} from {collection}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get document: {str(e)}")

@app.get("/collection_stats/{collection}")
async def get_collection_statistics(collection: str):
    """
    Get detailed statistics about a collection.
    """
    try:
        stats = await storage_manager.get_index_stats(collection)
        if stats:
            return stats
        else:
            raise HTTPException(status_code=404, detail="Collection not found")
    except Exception as e:
        logger.error(f"Failed to get collection stats for {collection}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get collection stats: {str(e)}")

@app.post("/search_by_metadata")
async def search_by_metadata(
    collection: str = Query(..., description="Collection name"),
    metadata_filter: str = Query(..., description="JSON string with metadata filters"),
    limit: int = Query(10, description="Number of documents to return")
):
    """
    Search documents by metadata filters.
    """
    try:
        # Not yet implemented in distributed manager
        raise NotImplementedError("Metadata search is not yet implemented in the distributed system.")
    except Exception as e:
        logger.error(f"Failed to search by metadata in {collection}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to search by metadata: {str(e)}")

# Distributed System Models
class NodeInfo(BaseModel):
    id: str
    host: str
    port: int
    status: str
    load: float
    vector_count: int
    collections: List[str]

class ClusterStatus(BaseModel):
    total_nodes: int
    healthy_nodes: int
    total_shards: int
    total_collections: int
    total_vectors: int
    replication_factor: int
    consistency_level: str
    nodes: List[NodeInfo]

class AddNodeRequest(BaseModel):
    node_id: str
    host: str
    port: int

# Distributed System Endpoints
@app.get("/cluster/status", response_model=ClusterStatus)
async def get_cluster_status():
    """
    Get the status of the distributed cluster.
    Shows node health, shard distribution, and system metrics.
    """
    try:
        # Get real cluster information from distributed storage manager
        cluster_info = await storage_manager.get_cluster_status()
        
        # Get all collections to show complete picture
        all_collections = await storage_manager.list_indexes()
        collection_names = [col["name"] for col in all_collections]
        
        # Build node information with real data
        nodes = []
        for node_info in cluster_info.get("nodes", []):
            # For now, we'll show all collections on each node since we can't easily
            # determine which collections exist on which specific nodes
            # In a real implementation, this would be tracked in the distributed store
            nodes.append(NodeInfo(
                id=node_info["id"],
                host=node_info["host"],
                port=node_info["port"],
                status=node_info["status"],
                load=node_info.get("load", 0.0),
                vector_count=node_info.get("vector_count", 0),
                collections=collection_names  # Show all collections for now
            ))
        
        return ClusterStatus(
            total_nodes=cluster_info.get("total_nodes", 3),
            healthy_nodes=cluster_info.get("healthy_nodes", 3),
            total_shards=cluster_info.get("total_shards", 8),
            total_collections=len(collection_names),
            total_vectors=cluster_info.get("total_vectors", 0),
            replication_factor=cluster_info.get("replication_factor", 2),
            consistency_level=cluster_info.get("consistency_level", "quorum"),
            nodes=nodes
        )
        
    except Exception as e:
        logger.error("Failed to get cluster status", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get cluster status: {str(e)}")

@app.post("/cluster/nodes")
async def add_node(request: AddNodeRequest):
    """
    Add a new node to the distributed cluster.
    """
    try:
        # In a real implementation, this would add the node to the distributed system
        logger.info(f"Adding node {request.node_id} at {request.host}:{request.port}")
        
        return {
            "status": "success",
            "message": f"Node {request.node_id} added successfully",
            "node": {
                "id": request.node_id,
                "host": request.host,
                "port": request.port,
                "status": "joining"
            }
        }
    except Exception as e:
        logger.error(f"Failed to add node: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to add node: {str(e)}")

@app.delete("/cluster/nodes/{node_id}")
async def remove_node(node_id: str):
    """
    Remove a node from the distributed cluster.
    """
    try:
        # In a real implementation, this would remove the node from the distributed system
        logger.info(f"Removing node {node_id}")
        
        return {
            "status": "success",
            "message": f"Node {node_id} removed successfully"
        }
    except Exception as e:
        logger.error(f"Failed to remove node {node_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to remove node: {str(e)}")

@app.get("/cluster/health")
async def cluster_health_check():
    """
    Comprehensive health check for the distributed cluster.
    """
    try:
        # Check individual node health
        node_health = []
        nodes = [
            {"id": "node1", "host": "localhost", "port": 8001},
            {"id": "node2", "host": "localhost", "port": 8002},
            {"id": "node3", "host": "localhost", "port": 8003}
        ]
        
        for node in nodes:
            try:
                response = requests.get(f"http://{node['host']}:{node['port']}/health", timeout=5)
                if response.status_code == 200:
                    node_health.append({
                        "node_id": node["id"],
                        "status": "healthy",
                        "response_time": response.elapsed.total_seconds()
                    })
                else:
                    node_health.append({
                        "node_id": node["id"],
                        "status": "unhealthy",
                        "error": f"HTTP {response.status_code}"
                    })
            except Exception as e:
                node_health.append({
                    "node_id": node["id"],
                    "status": "unreachable",
                    "error": str(e)
                })
        
        healthy_nodes = sum(1 for node in node_health if node["status"] == "healthy")
        total_nodes = len(node_health)
        
        return {
            "cluster_status": "healthy" if healthy_nodes >= total_nodes * 0.5 else "degraded",
            "healthy_nodes": healthy_nodes,
            "total_nodes": total_nodes,
            "node_health": node_health,
            "features": {
                "fault_tolerance": "enabled",
                "load_balancing": "enabled",
                "data_replication": "enabled",
                "consistency": "quorum"
            }
        }
    except Exception as e:
        logger.error(f"Cluster health check failed: {e}")
        raise HTTPException(status_code=500, detail=f"Cluster health check failed: {str(e)}")

@app.get("/cluster/sharding")
async def get_sharding_info():
    """
    Get detailed sharding information for the distributed cluster.
    Shows how data is distributed across nodes and shards.
    """
    try:
        cluster_status = await storage_manager.get_cluster_status()
        return {
            "sharding_strategy": "consistent_hashing",
            "shard_count": cluster_status.get("total_shards", 0),
            "replication_factor": cluster_status.get("replication_factor", 2),
            "consistency_level": cluster_status.get("consistency_level", "quorum"),
            "shard_distribution": cluster_status.get("shards", []),
            "node_distribution": cluster_status.get("nodes", [])
        }
    except Exception as e:
        logger.error("Failed to get sharding info", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get sharding info: {str(e)}")

# Auto-scaling endpoints
class AutoScalingStatus(BaseModel):
    is_running: bool
    last_scale_up: float
    last_scale_down: float
    scaling_history: List[Dict[str, Any]]
    current_metrics: Optional[Dict[str, Any]]
    thresholds: Dict[str, Any]

class UpdateThresholdsRequest(BaseModel):
    cpu_threshold_high: Optional[float] = None
    memory_threshold_high: Optional[float] = None
    storage_threshold_high: Optional[float] = None
    latency_threshold_high: Optional[float] = None
    error_rate_threshold_high: Optional[float] = None
    cpu_threshold_low: Optional[float] = None
    memory_threshold_low: Optional[float] = None
    storage_threshold_low: Optional[float] = None
    latency_threshold_low: Optional[float] = None
    error_rate_threshold_low: Optional[float] = None
    min_nodes: Optional[int] = None
    max_nodes: Optional[int] = None

@app.get("/autoscaling/status", response_model=AutoScalingStatus)
async def get_autoscaling_status():
    """
    Get the current status of the auto-scaler.
    Shows if it's running, recent scaling actions, and current metrics.
    """
    try:
        return auto_scaler.get_scaling_status()
    except Exception as e:
        logger.error("Failed to get auto-scaling status", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get auto-scaling status: {str(e)}")

@app.post("/autoscaling/start")
async def start_autoscaling():
    """
    Start the auto-scaling monitoring and scaling operations.
    """
    try:
        if not auto_scaler.is_running:
            asyncio.create_task(auto_scaler.start_monitoring())
            logger.info("Auto-scaling started")
            return {"status": "started", "message": "Auto-scaling monitoring started"}
        else:
            return {"status": "already_running", "message": "Auto-scaling is already running"}
    except Exception as e:
        logger.error("Failed to start auto-scaling", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to start auto-scaling: {str(e)}")

@app.post("/autoscaling/stop")
async def stop_autoscaling():
    """
    Stop the auto-scaling monitoring and scaling operations.
    """
    try:
        if auto_scaler.is_running:
            await auto_scaler.stop_monitoring()
            logger.info("Auto-scaling stopped")
            return {"status": "stopped", "message": "Auto-scaling monitoring stopped"}
        else:
            return {"status": "not_running", "message": "Auto-scaling is not running"}
    except Exception as e:
        logger.error("Failed to stop auto-scaling", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to stop auto-scaling: {str(e)}")

@app.post("/autoscaling/thresholds")
async def update_autoscaling_thresholds(request: UpdateThresholdsRequest):
    """
    Update auto-scaling thresholds dynamically.
    """
    try:
        # Get current thresholds
        current_thresholds = auto_scaler.thresholds
        
        # Update only provided values
        if request.cpu_threshold_high is not None:
            current_thresholds.cpu_threshold_high = request.cpu_threshold_high
        if request.memory_threshold_high is not None:
            current_thresholds.memory_threshold_high = request.memory_threshold_high
        if request.storage_threshold_high is not None:
            current_thresholds.storage_threshold_high = request.storage_threshold_high
        if request.latency_threshold_high is not None:
            current_thresholds.latency_threshold_high = request.latency_threshold_high
        if request.error_rate_threshold_high is not None:
            current_thresholds.error_rate_threshold_high = request.error_rate_threshold_high
        if request.cpu_threshold_low is not None:
            current_thresholds.cpu_threshold_low = request.cpu_threshold_low
        if request.memory_threshold_low is not None:
            current_thresholds.memory_threshold_low = request.memory_threshold_low
        if request.storage_threshold_low is not None:
            current_thresholds.storage_threshold_low = request.storage_threshold_low
        if request.latency_threshold_low is not None:
            current_thresholds.latency_threshold_low = request.latency_threshold_low
        if request.error_rate_threshold_low is not None:
            current_thresholds.error_rate_threshold_low = request.error_rate_threshold_low
        if request.min_nodes is not None:
            current_thresholds.min_nodes = request.min_nodes
        if request.max_nodes is not None:
            current_thresholds.max_nodes = request.max_nodes
        
        # Update the auto-scaler
        auto_scaler.update_thresholds(current_thresholds)
        
        logger.info("Updated auto-scaling thresholds")
        return {
            "status": "updated",
            "message": "Auto-scaling thresholds updated",
            "new_thresholds": auto_scaler.get_scaling_status()["thresholds"]
        }
    except Exception as e:
        logger.error("Failed to update auto-scaling thresholds", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to update thresholds: {str(e)}")

@app.post("/autoscaling/scale-up")
async def manual_scale_up():
    """
    Manually trigger a scale-up operation.
    Useful for testing or immediate scaling needs.
    """
    try:
        # Get current metrics
        metrics = await auto_scaler._collect_metrics()
        
        # Check if we can scale up
        if metrics.node_count >= auto_scaler.thresholds.max_nodes:
            raise HTTPException(status_code=400, detail="Maximum number of nodes reached")
        
        # Trigger scale up
        await auto_scaler._scale_up("Manual scale-up triggered", metrics)
        
        return {
            "status": "scaled_up",
            "message": f"Manually scaled up to {metrics.node_count + 1} nodes",
            "new_node_count": metrics.node_count + 1
        }
    except Exception as e:
        logger.error("Failed to manually scale up", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to scale up: {str(e)}")

@app.post("/autoscaling/scale-down")
async def manual_scale_down():
    """
    Manually trigger a scale-down operation.
    Useful for testing or immediate scaling needs.
    """
    try:
        # Get current metrics
        metrics = await auto_scaler._collect_metrics()
        
        # Check if we can scale down
        if metrics.node_count <= auto_scaler.thresholds.min_nodes:
            raise HTTPException(status_code=400, detail="Minimum number of nodes reached")
        
        # Trigger scale down
        await auto_scaler._scale_down("Manual scale-down triggered", metrics)
        
        return {
            "status": "scaled_down",
            "message": f"Manually scaled down to {metrics.node_count - 1} nodes",
            "new_node_count": metrics.node_count - 1
        }
    except Exception as e:
        logger.error("Failed to manually scale down", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to scale down: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(
        "src.api.main:app",
        host=settings.api.host,
        port=settings.api.port,
        reload=settings.api.reload,
        workers=settings.api.workers
    ) 