# üèóÔ∏è Distributed Vector Storage Architecture

## Overview

This project demonstrates a **true distributed architecture** for vector storage, replacing Qdrant with our own custom distributed system. This showcases real distributed systems concepts including fault tolerance, scalability, consistency, and load balancing.

## üéØ Key Features

### **1. Distributed Storage Nodes**
- **Multiple independent nodes** running on different ports
- **Individual FastAPI servers** for each storage node
- **Health monitoring** and load balancing
- **Automatic failover** when nodes go down

### **2. Fault Tolerance**
- **Data replication** across multiple nodes
- **Quorum-based consistency** (configurable)
- **Automatic recovery** from node failures
- **Graceful degradation** when nodes are unavailable

### **3. Scalability**
- **Horizontal scaling** - add/remove nodes dynamically
- **Shard distribution** across nodes using consistent hashing
- **Load balancing** with automatic rebalancing
- **Elastic capacity** based on demand

### **4. Consistency Management**
- **Configurable consistency levels**: ONE, QUORUM, ALL
- **Eventual consistency** with conflict resolution
- **Write-ahead logging** for durability
- **Distributed transactions** for complex operations

## üèõÔ∏è Architecture Components

### **1. Vector Storage Nodes (`src/storage/vector_node_server.py`)**
```python
# Individual node server
class VectorNodeServer:
    - Health monitoring
    - Vector storage and retrieval
    - Load reporting
    - Collection management
    - REST API endpoints
```

**Endpoints per node:**
- `GET /health` - Health check
- `POST /collections` - Create collection
- `DELETE /collections/{name}` - Delete collection
- `POST /vectors` - Upsert vectors
- `POST /search` - Search vectors
- `GET /collections` - List collections
- `GET /shards` - List shards
- `GET /stats` - Node statistics

### **2. Distributed Vector Store (`src/storage/distributed_vector_store.py`)**
```python
# Distributed coordination layer
class DistributedVectorStore:
    - Node management
    - Shard distribution
    - Load balancing
    - Health monitoring
    - Consistency management
```

### **3. Distributed Storage Manager (`src/storage/distributed_storage_manager.py`)**
```python
# High-level interface
class DistributedStorageManager:
    - Same interface as original storage manager
    - Transparent distributed operations
    - Fault tolerance handling
    - Cluster management
```

### **4. Storage Manager Integration (`src/storage/storage_manager.py`)**
```python
# Updated to use distributed system
class StorageManager:
    - Distributed vector storage
    - Legacy storage systems (Cassandra, Elasticsearch, MinIO)
    - Unified interface
```

## üöÄ Getting Started

### **1. Start the Distributed Nodes**
```bash
# Start all vector storage nodes
python start_vector_nodes.py
```

This starts 3 nodes:
- **Node 1**: `http://localhost:8001`
- **Node 2**: `http://localhost:8002` 
- **Node 3**: `http://localhost:8003`

### **2. Test the Distributed System**
```bash
# Run comprehensive tests
python test_distributed_system.py
```

### **3. Start the Main API**
```bash
# Start the main application
python -m src.api.main
```

## üìä Distributed System Endpoints

### **Cluster Management**
- `GET /cluster/status` - Get cluster status and node information
- `GET /cluster/health` - Comprehensive health check
- `GET /cluster/sharding` - Shard distribution information
- `POST /cluster/nodes` - Add a new node
- `DELETE /cluster/nodes/{node_id}` - Remove a node

### **Example Cluster Status Response**
```json
{
  "total_nodes": 3,
  "healthy_nodes": 3,
  "total_shards": 8,
  "total_collections": 5,
  "total_vectors": 1000,
  "replication_factor": 2,
  "consistency_level": "quorum",
  "nodes": [
    {
      "id": "node1",
      "host": "localhost",
      "port": 8001,
      "status": "healthy",
      "load": 0.3,
      "vector_count": 350,
      "collections": ["index_document", "index_image"]
    }
  ]
}
```

## üîß Configuration

### **Distributed System Settings**
```python
# In src/storage/storage_manager.py
class StorageConfig:
    enable_distributed_vector: bool = True
    replication_factor: int = 2
    consistency_level: str = "quorum"  # "one", "quorum", "all"
    shard_count: int = 8
    vector_size: int = 384
```

### **Node Configuration**
```python
# Default nodes (can be customized)
nodes = [
    VectorNode("node1", "localhost", 8001),
    VectorNode("node2", "localhost", 8002),
    VectorNode("node3", "localhost", 8003)
]
```

## üõ°Ô∏è Fault Tolerance Features

### **1. Node Failure Handling**
- **Automatic detection** of failed nodes
- **Request routing** to healthy nodes
- **Data availability** through replication
- **Automatic recovery** when nodes come back online

### **2. Data Replication**
- **Configurable replication factor** (default: 2)
- **Cross-node data distribution**
- **Automatic replica placement**
- **Consistency guarantees**

### **3. Load Balancing**
- **Request distribution** across nodes
- **Load monitoring** and reporting
- **Automatic rebalancing** of shards
- **Performance optimization**

## üìà Scalability Features

### **1. Horizontal Scaling**
```bash
# Add a new node
curl -X POST "http://localhost:8000/cluster/nodes" \
  -H "Content-Type: application/json" \
  -d '{"node_id": "node4", "host": "localhost", "port": 8004}'
```

### **2. Shard Distribution**
- **Consistent hashing** for shard assignment
- **Automatic shard redistribution** when nodes are added/removed
- **Load-aware shard placement**
- **Minimal data movement** during scaling

### **3. Performance Optimization**
- **Parallel processing** across nodes
- **Caching strategies** at node level
- **Connection pooling** for efficiency
- **Async operations** for better throughput

## üîç Monitoring and Observability

### **1. Health Monitoring**
```bash
# Check individual node health
curl http://localhost:8001/health
curl http://localhost:8002/health
curl http://localhost:8003/health

# Check cluster health
curl http://localhost:8000/cluster/health
```

### **2. Metrics and Statistics**
- **Node-level metrics**: load, vector count, request rate
- **Cluster-level metrics**: total vectors, shard distribution
- **Performance metrics**: response times, throughput
- **Health metrics**: node status, error rates

### **3. Logging and Debugging**
- **Structured logging** across all components
- **Request tracing** for distributed operations
- **Error reporting** with context
- **Performance profiling** capabilities

## üß™ Testing the Distributed System

### **1. Basic Functionality Test**
```bash
python test_distributed_system.py
```

### **2. Fault Tolerance Test**
```bash
# Start nodes
python start_vector_nodes.py

# In another terminal, kill a node process
# The system should continue operating

# Check cluster health
curl http://localhost:8000/cluster/health
```

### **3. Load Testing**
```bash
# Upload multiple documents
# Monitor node load distribution
# Check automatic load balancing
```

## üéì Academic Value

This implementation demonstrates key distributed systems concepts:

### **1. Distributed Algorithms**
- **Consistent hashing** for shard distribution
- **Quorum-based consensus** for consistency
- **Leader election** for coordination
- **Gossip protocols** for node communication

### **2. System Design Principles**
- **Fault tolerance** through replication
- **Scalability** through horizontal scaling
- **Consistency** through configurable levels
- **Availability** through redundancy

### **3. Real-World Patterns**
- **Microservices architecture**
- **Event-driven communication**
- **Circuit breaker patterns**
- **Retry mechanisms**

## üîÆ Future Enhancements

### **1. Advanced Features**
- **Distributed transactions** with 2PC/3PC
- **Conflict resolution** for concurrent updates
- **Compression** and deduplication
- **Backup and recovery** strategies

### **2. Production Readiness**
- **Persistent storage** (currently in-memory)
- **Security** and authentication
- **Monitoring** and alerting
- **Deployment** automation

### **3. Performance Optimizations**
- **Vector indexing** algorithms (HNSW, IVF)
- **Caching layers** (Redis, Memcached)
- **Connection pooling** and multiplexing
- **Batch processing** capabilities

## üìö References

- **Distributed Systems**: Concepts and Design (Coulouris et al.)
- **Designing Data-Intensive Applications** (Martin Kleppmann)
- **Consistent Hashing** (Karger et al.)
- **Raft Consensus Algorithm** (Ongaro & Ousterhout)

---

This distributed architecture demonstrates how to build a **production-ready, scalable vector storage system** that can handle real-world workloads with fault tolerance and high availability. It's perfect for academic projects showing distributed systems concepts in action! üéâ 